# Dr-Bot-LLM-Clinical-Assistant
An End-to-end LLM project named as **clinical assistant**. Classifies patient questions with PubMedBERT fine-tuned on MedQuAD, retrieves background from wikipedia, and generates clear, empathetic answers with Mistral-7B-Instruct LLM model. Targeted towards safety and low-hallucination guidance.

## üìö Table of Contents

- [Overview](#-overview)
- [Aim & Objectives](#-aim-and-objectives)
- [Dataset Description](#-dataset-description)
- [System Design](#-system-design)
- [Challenges & Fixes](#-challenges-and-fixes)
- [Contact Information](#-contact-information)

---

## üîç Overview

This LLM project was designed towards a **patient-centered LLM pipeline** that takes a patient‚Äôs text health query and generates a clear, medically grounded, and human-like response. The patient's question is first classified into the medical domain using a PubMedBERT, which would further fine-tuned on the MedQuAD dataset. Then, the classifier identifies the core topic, and the system routes the query to a curated offline Wikipedia snapshot relevant to that topic. From there, it extracts key information such as definitions, red flags, self-care steps, and when to consult a doctor. A final response is generated by Mistral 7B which was tailored response specific to patient query.

---

## üìä Aim & Objectives

1) **Aim** : Develop an LLM-powered clinical assistant (Dr-Bot) that can receive real, patient-initiated questions and return concise, medically grounded, and empathetic answers‚Äîsafely. The system should be explainable, reproducible, and easy to evaluate by patients and clinicians.

2) **Objectives** :
    1. **Understand & route**: Classify each patient query into the correct medical focus (PubMedBERT/MedQuAD) and route the pipeline accordingly.
    2. **Ground with offline knowledge**: Retrieve concise background from a curated offline Wikipedia snapshot to minimize hallucinations.
    3. **Generate safe, empathetic answers**: Use Mistral-7B-Instruct to produce one short, patient-friendly paragraph aligned with best practices.
    4. **Operate reliably & reproducibly**: Run with memory-aware settings on Kaggle and expose a simple, testable Model.predict() package.

--- 

## üß© Dataset Description
The data for this project was sourced from the MedQuAD dataset **[Kaggle](https://www.kaggle.com/datasets/jpmiller/layoutlm?resource=download)**. This medical question answering dataset is a curated corpus of patient-oriented Q&A pairs collected from authoritative U.S. health sources. This processed split contains ~15k questions with matched answers, a focus_area (disease/condition/topic) label, and the original source. It‚Äôs well-suited for intent/focus classification, retrieval-augmented generation, and safety-aware answer synthesis. Use for research/education and respect the source sites‚Äô terms.

---

## üß© System Design

![system-design-visual](https://github.com/user-attachments/assets/03f757a9-1d19-453d-8fb5-c874b144c295)

1) **Input ‚Üí** Patient query free-text concern from the user.

2) **PubMedBERT classifier (two-stage)**
    1. **Base encoder:** PubMedBERT turns the query into a medical embedding.
    2. **Secondary head:** label-embedding + temperature (œÑ) picks the focus area (e.g., Hypertension).

3) **Offline knowledge retrieval (Wiki data)** For the predicted focus area, the matching local .txt article loaded and split into sections to extract the background information for that area which is classfied.

4) **Context building (compact & transparent)** Developed short previews and then condense the article to a small token budget (~450 tokens) to keep latency low while staying grounded.

5) **Physician generator (Mistral-7B-Instruct)** At the end, the LLM model receives: Focus area which is classified + Background (condensed wikpedia content) + patient question, and produces a single tailored response.

6) **Output ‚Üí** A concise, empathetic, clinically appropriate response.


---

## üí° Challenges and Fixes

1) **Network-Restricted Runtime**
   - ***Challenge***: Kaggle evaluation runs with internet disabled; external knowledge/lookups are disallowed.
   - ***Fix***: Shipped curated, pre-processed offline Wikipedia briefs per focus area with the notebook. predict() stays fully grounded without network access.

2) **Memory Constraints & Kernel Stability**
   - ***Challenge***: Limited RAM caused tokenizer/model memory pressure and occasional kernel restarts.
   - ***Fix***: Split work into two notebooks (model dev vs. package export), applied quantization and tight token budgets, enabling all artifacts to fit within Kaggle RAM.

---

## ‚ú® Contact Information

üì¨ Contact Author: Dhyey Kasundra <br> 
üìß Email: dhyey.d.kasundra@gmail.com

üí° When data listens, healthcare answers.

---
