{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dr.Bot Project\n",
    "\n",
    "## Exporting to Kaggle Package\n",
    "\n",
    "Here we will setup the code to initialize the model class by pulling the base model then installing our trained weights, which will further carry-forward to testing of the package in LLM-Output code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-12T18:19:45.105956Z",
     "iopub.status.busy": "2025-08-12T18:19:45.105609Z",
     "iopub.status.idle": "2025-08-12T18:19:45.131652Z",
     "shell.execute_reply": "2025-08-12T18:19:45.130715Z",
     "shell.execute_reply.started": "2025-08-12T18:19:45.105890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# ===== Kaggle Package: package.Model =====\n",
    "# Single-class package with .predict(str) -> str\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import random\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig \n",
    "from cachetools import TTLCache, cached\n",
    "import os, re, unicodedata, pickle, torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "import kagglehub\n",
    "\n",
    "\n",
    "# --- small helpers (no external deps) ---\n",
    "def _norm(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    return \"\".join(ch.lower() for ch in s if ch.isalnum())\n",
    "\n",
    "def _finalize_paragraph(s: str) -> str:\n",
    "    s = s.replace(\"\\n\", \" \").strip()\n",
    "    s = re.sub(r\"(^|\\s)(?:\\d+[\\.\\)]|[-*•])\\s+\", \" \", s)     # remove numbering/bullets\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    last = max(s.rfind(\".\"), s.rfind(\"?\"), s.rfind(\"!\"))\n",
    "    return s[: last + 1] if last != -1 else (s.rstrip(\",;:- \") + \".\")\n",
    "\n",
    "class LabelEmbCls(torch.nn.Module):\n",
    "    \"\"\"BERT encoder + frozen label-embedding head with temperature τ.\"\"\"\n",
    "    def __init__(self, base, lbl_emb):\n",
    "        super().__init__()\n",
    "        self.bert = base\n",
    "        self.lbl_E = torch.nn.Parameter(lbl_emb, requires_grad=False)\n",
    "        self.tau   = torch.nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        cls = out.last_hidden_state[:, 0]  # [CLS]\n",
    "        return torch.matmul(cls, self.lbl_E.T) / self.tau\n",
    "\n",
    "class Model:\n",
    "    \"\"\"\n",
    "    Submission package.\n",
    "    Usage (grader):\n",
    "        package = kagglehub.package_import('your-username/your-notebook/versions/X')\n",
    "        model = package.Model()\n",
    "        print(model.predict(\"I have a headache...\"))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        artifact_dir=None,\n",
    "        backbone_dir=None,\n",
    "        wiki_dir=None,\n",
    "        phys_dir=None,\n",
    "        prefix=\"SeverityNormal -- \",\n",
    "        max_len=64,\n",
    "        ctx_bg_token_budget=450,\n",
    "        max_new_tokens=120,\n",
    "        min_new_tokens=70,\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.prefix = prefix\n",
    "        self.max_len = max_len\n",
    "        self.ctx_bg_token_budget = ctx_bg_token_budget\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.min_new_tokens = min_new_tokens\n",
    "\n",
    "        # --- resolve inputs via kagglehub (NO /kaggle/input anywhere) ---\n",
    "        if artifact_dir is None:\n",
    "            artifact_dir = kagglehub.model_download(\"dhyeyk29/pubmedbert_model/PyTorch/default/1\")\n",
    "        if backbone_dir is None:\n",
    "            backbone_dir = kagglehub.model_download(\"dhyeyk29/pubmedbert_base/PyTorch/default/1\")\n",
    "        if wiki_dir is None:\n",
    "            wiki_dir = kagglehub.dataset_download(\"dhyeyk29/wikipedia-data\")\n",
    "        if phys_dir is None:\n",
    "            phys_dir = kagglehub.model_download(\"dhyeyk29/physician_transformer/PyTorch/default/1\") \n",
    "\n",
    "        # --- build wiki index once (fast lookup) ---\n",
    "        self.wiki_index = {}\n",
    "        if os.path.isdir(wiki_dir):\n",
    "            for root, _, files in os.walk(wiki_dir):\n",
    "                for fn in files:\n",
    "                    if fn.lower().endswith(\".txt\"):\n",
    "                        self.wiki_index[_norm(Path(fn).stem)] = os.path.join(root, fn)\n",
    "\n",
    "        # --- load classifier artifacts (offline) ---\n",
    "        self.tok = AutoTokenizer.from_pretrained(artifact_dir, local_files_only=True)\n",
    "        self.bert = AutoModel.from_pretrained(backbone_dir, local_files_only=True).to(self.device).eval()\n",
    "        label_embs = torch.load(os.path.join(artifact_dir, \"label_embs.pt\"),\n",
    "                                map_location=self.device).to(self.device)\n",
    "        with open(os.path.join(artifact_dir, \"id2label.pkl\"), \"rb\") as f:\n",
    "            self.id2label = pickle.load(f)\n",
    "\n",
    "        self.cls_model = LabelEmbCls(self.bert, label_embs).to(self.device)\n",
    "        state = torch.load(os.path.join(artifact_dir, \"classifier.pt\"), map_location=self.device)\n",
    "        if isinstance(state, dict) and any(k.startswith(\"module.\") for k in state.keys()):\n",
    "            state = {k.replace(\"module.\", \"\"): v for k, v in state.items()}\n",
    "        self.cls_model.load_state_dict(state, strict=False)\n",
    "        self.cls_model.eval()\n",
    "\n",
    "        # --- load physician LLM (guarded quantization) ---\n",
    "        self.phys_tok = AutoTokenizer.from_pretrained(phys_dir, local_files_only=True)\n",
    "        load_kwargs = dict(local_files_only=True, low_cpu_mem_usage=True)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            # try 4-bit; fall back to fp16 if bitsandbytes not available\n",
    "            try:\n",
    "                from transformers import BitsAndBytesConfig\n",
    "                bnb_cfg = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "                    bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16\n",
    "                )\n",
    "                load_kwargs.update(dict(quantization_config=bnb_cfg, device_map=\"auto\"))\n",
    "            except Exception:\n",
    "                load_kwargs.update(dict(torch_dtype=torch.float16, device_map=\"auto\"))\n",
    "        else:\n",
    "            load_kwargs.update(dict(torch_dtype=torch.float32, device_map={\"\": \"cpu\"}))\n",
    "\n",
    "        self.phys_model = AutoModelForCausalLM.from_pretrained(phys_dir, **load_kwargs).eval()\n",
    "\n",
    "        # tokenizer/model hygiene\n",
    "        if self.phys_tok.eos_token_id is None:\n",
    "            self.phys_tok.add_special_tokens({\"eos_token\": \"</s>\"})\n",
    "            self.phys_model.resize_token_embeddings(len(self.phys_tok))\n",
    "        if self.phys_tok.pad_token_id is None:\n",
    "            self.phys_tok.pad_token = self.phys_tok.eos_token\n",
    "        self.phys_model.config.eos_token_id = self.phys_tok.eos_token_id\n",
    "        self.phys_model.config.pad_token_id = self.phys_tok.pad_token_id\n",
    "        self.phys_tok.truncation_side = \"left\"\n",
    "\n",
    "    # ---- private helpers ----\n",
    "    def _classify(self, q: str) -> str:\n",
    "        enc = self.tok(self.prefix + q, truncation=True, max_length=self.max_len,\n",
    "                       padding=\"max_length\", return_tensors=\"pt\")\n",
    "        enc = {k: v.to(self.device) for k, v in enc.items()}\n",
    "        with torch.no_grad():\n",
    "            logits = self.cls_model(**enc)\n",
    "            pred_id = int(torch.argmax(logits, dim=-1).item())\n",
    "        return self.id2label[pred_id]\n",
    "\n",
    "    def _get_background(self, focus: str) -> str:\n",
    "        path = self.wiki_index.get(_norm(focus))\n",
    "        if not path: return \"\"\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                return f.read()\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    def _clamp_by_tokens(self, text: str, budget: int) -> str:\n",
    "        ids = self.phys_tok.encode(text, add_special_tokens=False)\n",
    "        return text if len(ids) <= budget else self.phys_tok.decode(ids[:budget], skip_special_tokens=True)\n",
    "\n",
    "    # ---- public API ----\n",
    "    def predict(self, question: str) -> str:\n",
    "        \"\"\"Takes a single question string, returns a single response string.\"\"\"\n",
    "        question = (question or \"\").strip()\n",
    "        if not question:\n",
    "            return \"Please provide a short description of your concern.\"\n",
    "\n",
    "        # 1) classify\n",
    "        focus = self._classify(question)\n",
    "\n",
    "        # 2) retrieve background (optional but preferred)\n",
    "        bg = self._get_background(focus)\n",
    "        context = self._clamp_by_tokens(bg, self.ctx_bg_token_budget) if bg else \"\"\n",
    "\n",
    "        # 3) prompt & generate (single short paragraph)\n",
    "        prompt = (\n",
    "            \"You are a board-certified physician. Using ONLY the background below, write ONE short paragraph \"\n",
    "            \"(4–5 sentences). Be empathetic, give practical next steps, and mention urgent-care signs only if warranted. \"\n",
    "            \"Do NOT use bullet points, numbering, or line breaks.\\n\\n\"\n",
    "            f\"Background:\\n{context}\\n\\n\"\n",
    "            f\"User question: {question}\\n\"\n",
    "            \"Answer (one short paragraph, no lists or numbering):\\n\"\n",
    "        )\n",
    "\n",
    "        inputs = self.phys_tok(prompt, return_tensors=\"pt\", truncation=True)  # no padding\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            gen = self.phys_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                min_new_tokens=self.min_new_tokens,\n",
    "                do_sample=False,\n",
    "                use_cache=True,\n",
    "                pad_token_id=self.phys_tok.pad_token_id,\n",
    "                eos_token_id=self.phys_tok.eos_token_id,\n",
    "            )\n",
    "\n",
    "        text = self.phys_tok.decode(gen[0], skip_special_tokens=True)\n",
    "        ans  = text.split(\"Answer\", 1)[-1]\n",
    "        ans  = ans.split(\":\", 1)[-1] if \":\" in ans else ans\n",
    "        return _finalize_paragraph(ans)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7815660,
     "sourceId": 12394315,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8057696,
     "sourceId": 12746441,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8057720,
     "sourceId": 12746475,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 1902,
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 426472,
     "modelInstanceId": 408614,
     "sourceId": 518825,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 426473,
     "modelInstanceId": 408615,
     "sourceId": 518827,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
